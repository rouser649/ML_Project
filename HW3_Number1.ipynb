{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dddd38c",
   "metadata": {},
   "source": [
    "## Question 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3336de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#Packages\n",
    "using Gurobi\n",
    "using JuMP\n",
    "using Plots\n",
    "using GLMNet\n",
    "using DataFrames \n",
    "using CSV\n",
    "using Statistics\n",
    "using Random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "08960913",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#Load in the data \n",
    "\n",
    "xtrain_filepath = \"airfoil_X_train.csv\"\n",
    "xtest_filepath = \"airfoil_X_test.csv\"\n",
    "\n",
    "xtrain = Matrix(CSV.read(xtrain_filepath,DataFrame)) \n",
    "xtest = Matrix(CSV.read(xtest_filepath,DataFrame)) \n",
    "\n",
    "xtrain = Array{Float64,2}(xtrain)\n",
    "xtest = Array{Float64,2}(xtest);\n",
    "\n",
    "\n",
    "#Y data\n",
    "ytrain_filepath = \"airfoil_Y_train.csv\"\n",
    "ytest_filepath = \"airfoil_Y_test.csv\"\n",
    "\n",
    "ytrain = Matrix(CSV.read(ytrain_filepath, DataFrame))\n",
    "ytest = Matrix(CSV.read(ytest_filepath, DataFrame));\n",
    "\n",
    "ytrain = Vector(ytrain[:,1])\n",
    "ytest = Vector(ytest[:,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5ae9378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#Rsquared function\n",
    "function r_squared(y_pred, data)\n",
    "    \n",
    "    \n",
    "    error = (ytest - y_pred).^2\n",
    "    sse = sum(error)\n",
    "    \n",
    "    \n",
    "    total = []\n",
    "    for i in 1:size(ytest)[1]\n",
    "        error = (ytest[i] - mean(ytrain))^2\n",
    "        push!(total, error)\n",
    "        \n",
    "    end\n",
    "    \n",
    "    sst = sum(total)\n",
    "    r_sq = 1 - sse/sst\n",
    "    \n",
    "    return r_sq\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a277289",
   "metadata": {},
   "source": [
    "# Part A \n",
    "### Run Regularized Lasso Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f54e81d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6382299231669778"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################\n",
    "#### Part A ####\n",
    "lasso_reg = glmnetcv(xtrain, ytrain, intercept = false)\n",
    "y_pred = predict(lasso_reg, xtest)\n",
    "\n",
    "r_squared(y_pred, xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b4d12",
   "metadata": {},
   "source": [
    "Out of sample r_squared is 0.637892588370812\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64688c40",
   "metadata": {},
   "source": [
    "# Part B\n",
    "\n",
    "### Heuristic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b32b3867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation between column 1 and column 8 is: 0.9445551179417134\n",
      "The correlation between column 2 and column 5 is: 0.7624911361809463\n",
      "The correlation between column 2 and column 11 is: 0.909714791094822\n",
      "The correlation between column 2 and column 12 is: 0.8322778171108243\n",
      "The correlation between column 2 and column 15 is: 0.7294389171977992\n",
      "The correlation between column 3 and column 13 is: 0.8814653995040456\n",
      "The correlation between column 5 and column 12 is: 0.9622678350886917\n",
      "The correlation between column 5 and column 14 is: 0.8177688318583504\n",
      "The correlation between column 5 and column 15 is: 0.9269744117722346\n",
      "The correlation between column 6 and column 9 is: 0.7721438010525238\n",
      "The correlation between column 10 and column 14 is: 0.8090659626914039\n",
      "The correlation between column 11 and column 12 is: 0.7498124664425927\n",
      "The correlation between column 11 and column 15 is: 0.7861025290570582\n",
      "The correlation between column 12 and column 15 is: 0.9018212269618643\n",
      "The correlation between column 14 and column 15 is: 0.7554826568855731\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "### Find the correlations of all the variables. ####\n",
    "correlations = []\n",
    "col_i = []\n",
    "col_j = []\n",
    "for i in 1:size(xtrain)[2]\n",
    "    for j in i+1:size(xtrain)[2]\n",
    "        \n",
    "        cov_ij = cov(xtrain[:,i], xtrain[:,j])\n",
    "        var_i = var(xtrain[:,i])\n",
    "        var_j = var(xtrain[:,j])\n",
    "\n",
    "        cor_ij = cov_ij /(var_i *var_j)^.5\n",
    "        \n",
    "        push!(correlations, cor_ij)\n",
    "        \n",
    "        if cor_ij >= 0.7 \n",
    "            \n",
    "            print(\"\")\n",
    "            println(\"The correlation between column \", i, \" and column \", j, \" is: \", cor_ij)\n",
    "            push!(col_i, i)\n",
    "            push!(col_j, j)\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "#Identify the columns I want to keep. \n",
    "idx = [1,2,3,4,6,7,8,10]\n",
    "\n",
    "#Subset the columns\n",
    "x_subset = xtrain[:,idx]\n",
    "\n",
    "#Perform lasso on the new x. \n",
    "subset_reg = glmnetcv(x_subset, ytrain, intercept = false)\n",
    "y_pred = predict(subset_reg, xtest);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cfcbcfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Out of sample accuracy is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18319306828007187"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println()\n",
    "println(\"Out of sample accuracy is\")\n",
    "r_squared(y_pred, xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e904ae",
   "metadata": {},
   "source": [
    "# Part C\n",
    "### Implement Holistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fbf2df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "function holistic(M, gamma, k,  X, Y, x_test, y_val)\n",
    "    #Initialize the model.\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, gurobi_env))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "\n",
    "\n",
    "\n",
    "    #Get dimensions of x_tilda\n",
    "    n, p = size(X)  \n",
    "\n",
    "    #set up the variables\n",
    "    @variable(model, beta[j = 1:p])\n",
    "    @variable(model, z[j = 1:p], Bin)\n",
    "    @variable(model, t[j = 1:p] >= 0) #l1 norm\n",
    "\n",
    "\n",
    "    #sparsity contraint\n",
    "    @constraint(model, [i = 1:p], -M * z[i] <= beta[i])\n",
    "    @constraint(model, [i = 1:p], beta[i] <= M*z[i])\n",
    "\n",
    "    #sparsity contraint \n",
    "    @constraint(model, sum(z) <= k)\n",
    "\n",
    "    groups = correlated_variables(X)\n",
    "    \n",
    "    # correlation constraint\n",
    "    for g in 1:size(groups)[1]\n",
    "        idx_1 = groups[g,1]\n",
    "        idx_2 = groups[g,2]\n",
    "        @constraint(model, z[idx_1] + z[idx_2] <= 1)\n",
    "    end\n",
    "\n",
    "\n",
    "    #group sparsity \n",
    "    @constraint(model, [i= 1:size(xtrain)[2]], z[4*(i-1)+1] +z[4*(i-1)+2] + z[4*(i-1)+3] + z[4*(i-1)+4] <= 1)\n",
    "\n",
    "    #Handle the l1 norm\n",
    "    @constraint(model, [i=1:p], beta[i] <= t[i])\n",
    "    @constraint(model, [i=1:p], -beta[i] <= t[i])\n",
    "\n",
    "    #0.5* sum((ytrain - x_tilda * beta).^2)\n",
    "\n",
    "\n",
    "    ## Objective Function \n",
    "    @objective(model, Min, 0.5 * sum((Y - X * beta).^2) + gamma*sum(t))\n",
    "\n",
    "    optimize!(model)\n",
    "\n",
    "    betas = value.(beta)\n",
    "\n",
    "    \n",
    "    y_pred = x_test * betas \n",
    "\n",
    "    #r = r_squared(y_val, y_pred, ytrain)\n",
    "    r = r_squared(y_val, y_pred, Y)\n",
    "    \n",
    "    mse = compute_mse(x_test,y_val, betas )\n",
    "   \n",
    "    return(r, betas, objective_value(model), mse)\n",
    "    \n",
    "\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ed198c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a function to identify correlated variables\n",
    "function correlated_variables(X)\n",
    "    #Get correlation matrix \n",
    "    cx = cor(X)\n",
    "\n",
    "    #Get the ones indices of the variables that are greater than 0.7\n",
    "    ci = []\n",
    "    cj = []\n",
    "    for i in 1:size(X)[1]\n",
    "\n",
    "        for j in i+1:size(X)[2]\n",
    "            \n",
    "            if cx[i,j] >= 0.7\n",
    "                push!(ci, i)\n",
    "                push!(cj, j)\n",
    "            end\n",
    "\n",
    "        end\n",
    "\n",
    "    end\n",
    "\n",
    "    groups = hcat(ci, cj)\n",
    "    return(groups)\n",
    "\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f8f36be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rsquared function\n",
    "function r_squared(y_true, y_pred, training_y)\n",
    "    \n",
    "    error = (y_true - y_pred).^2\n",
    "    sse = sum(error)\n",
    "    \n",
    "    \n",
    "    total = []\n",
    "    for i in 1:size(y_true)[1]\n",
    "        error = (y_true[i] - mean(training_y))^2\n",
    "        push!(total, error)\n",
    "        \n",
    "    end\n",
    "    \n",
    "    sst = sum(total)\n",
    "    r_sq = 1 - sse/sst\n",
    "    \n",
    "    return r_sq\n",
    "    \n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bdfd9239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_mse (generic function with 1 method)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compute_mse(X, y, beta)\n",
    "    n,p = size(X)\n",
    "    return sum((X*beta .- y).^2)/n\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f912cdb6",
   "metadata": {},
   "source": [
    "### Transform the X data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f7616898",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "### Create the transformed data set x tilda for the test set ###\n",
    "\n",
    "n,p = size(xtest)\n",
    "eps = .0001\n",
    "x_test_tilda = Array{Float64}(undef, n, 0)\n",
    "\n",
    "#Loop through the original 15 columns\n",
    "for col in 1:p\n",
    "\n",
    "    #handle the intercept\n",
    "    # if col == 0\n",
    "    #     int = ones(n)\n",
    "    #     x_j = hcat(int)\n",
    "    # end\n",
    "    \n",
    "\n",
    "    if col !=0\n",
    "        x = xtest[:,col]\n",
    "        #Add the new transformed columns\n",
    "        x_j = hcat(x, x.^2, abs.(x).^.5, log.(abs.(x .+ eps)))\n",
    "    end\n",
    "\n",
    "    x_test_tilda = hcat(x_test_tilda, x_j)\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "#Transformation of xtrain\n",
    "#Get the dimensions of x\n",
    "n,p = size(xtrain)\n",
    "\n",
    "\n",
    "###############################################\n",
    "### Create the transformed data set x tilda ###\n",
    "\n",
    "x_tilda = Array{Float64}(undef, n, 0)\n",
    "\n",
    "#Loop through the original 15 columns\n",
    "for col in 1:p\n",
    "\n",
    "    #handle the intercept\n",
    "    # if col == 0\n",
    "    #     int = ones(n)\n",
    "    #     x_j = hcat(int)\n",
    "    # end\n",
    "    \n",
    "\n",
    "    if col !=0\n",
    "        x = xtrain[:,col]\n",
    "        #Add the new transformed columns\n",
    "        x_j = hcat(x, x.^2, abs.(x).^.5, log.(abs.(x .+ eps )))\n",
    "    end\n",
    "\n",
    "    x_tilda = hcat(x_tilda, x_j)\n",
    "\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63b67ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only - expires 2022-08-18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "191.24381715440592"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test to see if it works\n",
    "gurobi_env = Gurobi.Env()\n",
    "\n",
    "#M, gamma, k,  X, Y, x_test, y_val\n",
    "r, b, obj, mse = holistic(100, 0.5, 10,  x_tilda, ytrain, x_test_tilda, ytest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72078324",
   "metadata": {},
   "source": [
    "## Five fold cross validation for Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b82e9a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDataUtils\n",
    "\n",
    "n, p = size(xtrain)\n",
    "\n",
    "#Create the folds\n",
    "folds = kfolds(randperm(n), 5)\n",
    "\n",
    "#Set parameters\n",
    "M = 100\n",
    "k = 10\n",
    "gamma = collect(0:.1:2)\n",
    "#gamma = 0.1\n",
    "#f = 1\n",
    "avg_rsq = []\n",
    "avg_mse = []\n",
    "for gamma in gamma\n",
    "    rsq = []\n",
    "    mse_vec =[]\n",
    "    for f in 1:5\n",
    "\n",
    "        x_train_val = x_tilda[folds[f][1],:]\n",
    "        y_train_val = ytrain[folds[f][1],:]\n",
    "\n",
    "        x_val = x_tilda[folds[f][2],:]\n",
    "        y_val = ytrain[folds[f][2],:]\n",
    "  \n",
    "        r, b, obj, mse = holistic(100, gamma, 10, x_train_val, y_train_val, x_val, y_val)\n",
    "        push!(rsq, r)\n",
    "        push!(mse_vec, mse)\n",
    "\n",
    "\n",
    "    end\n",
    "\n",
    "    push!(avg_rsq, mean(rsq))\n",
    "    push!(avg_mse, mean(mse_vec))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b95eb3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gamma = 0.2\n",
      "Out of sample accuracy of the holistic regression is 0.6332716929612672\n"
     ]
    }
   ],
   "source": [
    "idx = argmin(avg_rsq)\n",
    "best_gamma = gamma[idx]\n",
    "\n",
    "r, b, obj, mse = holistic(100, 0.2, 10, x_tilda, ytrain, x_test_tilda, ytest)\n",
    "\n",
    "println(\"Using gamma = 0.2\")\n",
    "println(\"Out of sample accuracy of the holistic regression is \", r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c04fe0",
   "metadata": {},
   "source": [
    "The features that are non zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0307fca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Int64}:\n",
       "  1\n",
       "  5\n",
       "  9\n",
       " 13\n",
       " 25\n",
       " 33\n",
       " 42\n",
       " 50\n",
       " 54\n",
       " 58"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findall(b.!= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2529589f",
   "metadata": {},
   "source": [
    "# Part D\n",
    "\n",
    "### I had a hard time finding friends\n",
    "\n",
    "With the two friends I found, we had wildly different model perforamce for part B.\n",
    "The variables I selected for part B were variables that were not highly correlated and I had a preference to chose the original variables (no interaction terms). This model perforamce for me was quite poor. Both of my friends had better model performace but both chose different features to include in the model. \n",
    "\n",
    "For part C, all three of us got the almost identical out of sample performace (up to the 4th digit for r squared).\n",
    "\n",
    "\n",
    "Benefits of holistic regression include:\n",
    "- Robust way to identify which features to include in the data instead of randomly picking. \n",
    "- All of us had different ideas of why we included some variable over another in part b, there was no such disagreement in holistic. That being said, some of our betas were not identical (I am confused by this). \n",
    "- With holistic regression we are able to consider transformed variables. In large data sets it can be very difficult to decide how to transform variables because you have to do so many permutations of the data. Here we were easily able to consider 4 transformations, and this can easily scale to more. It turns out including some log transformed and squared transformed variables increase model performane. \n",
    "- For this problem, it is not readily apparent to me why some transformed variables were chosen over others. But maybe that is because the regression problem on noise is outside my realm of expertise. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
