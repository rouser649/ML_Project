{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48bc9c36",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90668b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `C:\\Users\\ultan\\.julia\\registries\\General`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\ultan\\.julia\\environments\\v1.6\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\ultan\\.julia\\environments\\v1.6\\Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\ultan\\.julia\\environments\\v1.6\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\ultan\\.julia\\environments\\v1.6\\Manifest.toml`\n",
      "`DataFrames` is pinned at `v1.1.1`: maintaining pinned version\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\ultan\\.julia\\environments\\v1.6\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\ultan\\.julia\\environments\\v1.6\\Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\ultan\\.julia\\environments\\v1.6\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\ultan\\.julia\\environments\\v1.6\\Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\ultan\\.julia\\environments\\v1.6\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\ultan\\.julia\\environments\\v1.6\\Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "\n",
    "Pkg.add(\"Random\")\n",
    "Pkg.add(\"Statistics\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"CSV\")\n",
    "Pkg.add(\"Plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4a26303",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, Tables\n",
    "using DataFrames\n",
    "using Statistics\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3f9114",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d33d5d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_all_train_val_proc = CSV.read(\"train_val_processed.csv\", DataFrame,header=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5b11c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Item_Weight</th><th>Item_Visibility</th><th>Item_MRP</th><th>Years_Opened</th><th>Item_Type_Baking Goods</th><th>Item_Type_Breads</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Int64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>6,392 rows × 48 columns (omitted printing of 42 columns)</p><tr><th>1</th><td>14.6</td><td>0.0594052</td><td>179.698</td><td>16</td><td>0</td><td>0</td></tr><tr><th>2</th><td>16.25</td><td>0.025777</td><td>169.947</td><td>14</td><td>0</td><td>0</td></tr><tr><th>3</th><td>12.7929</td><td>0.0907781</td><td>153.102</td><td>28</td><td>0</td><td>0</td></tr><tr><th>4</th><td>13.85</td><td>0.0259367</td><td>164.921</td><td>14</td><td>0</td><td>0</td></tr><tr><th>5</th><td>9.195</td><td>0.0518271</td><td>77.4644</td><td>4</td><td>1</td><td>0</td></tr><tr><th>6</th><td>12.7929</td><td>-0.969538</td><td>52.6666</td><td>28</td><td>0</td><td>0</td></tr><tr><th>7</th><td>14.65</td><td>0.0721787</td><td>262.959</td><td>14</td><td>0</td><td>0</td></tr><tr><th>8</th><td>9.6</td><td>0.161182</td><td>165.316</td><td>15</td><td>0</td><td>0</td></tr><tr><th>9</th><td>16.85</td><td>0.159826</td><td>195.548</td><td>9</td><td>0</td><td>0</td></tr><tr><th>10</th><td>6.03</td><td>0.0226994</td><td>176.403</td><td>9</td><td>0</td><td>0</td></tr><tr><th>11</th><td>12.7929</td><td>0.0269333</td><td>78.467</td><td>28</td><td>0</td><td>0</td></tr><tr><th>12</th><td>11.6</td><td>0.0387368</td><td>55.8272</td><td>6</td><td>0</td><td>0</td></tr><tr><th>13</th><td>12.7929</td><td>0.11225</td><td>123.341</td><td>28</td><td>0</td><td>0</td></tr><tr><th>14</th><td>12.7929</td><td>-0.969538</td><td>240.622</td><td>28</td><td>0</td><td>0</td></tr><tr><th>15</th><td>15.1</td><td>0.0259934</td><td>147.408</td><td>16</td><td>0</td><td>0</td></tr><tr><th>16</th><td>6.325</td><td>0.125688</td><td>100.904</td><td>4</td><td>0</td><td>0</td></tr><tr><th>17</th><td>12.6</td><td>0.0627978</td><td>103.999</td><td>14</td><td>0</td><td>0</td></tr><tr><th>18</th><td>12.7929</td><td>0.0652031</td><td>166.082</td><td>28</td><td>0</td><td>0</td></tr><tr><th>19</th><td>20.7</td><td>0.169776</td><td>184.427</td><td>9</td><td>0</td><td>0</td></tr><tr><th>20</th><td>10.8</td><td>0.0498199</td><td>243.214</td><td>11</td><td>0</td><td>0</td></tr><tr><th>21</th><td>12.3</td><td>0.00942961</td><td>71.338</td><td>11</td><td>1</td><td>0</td></tr><tr><th>22</th><td>8.155</td><td>0.119351</td><td>190.153</td><td>26</td><td>0</td><td>0</td></tr><tr><th>23</th><td>12.7929</td><td>-0.969538</td><td>171.742</td><td>28</td><td>0</td><td>0</td></tr><tr><th>24</th><td>7.905</td><td>0.0100279</td><td>249.641</td><td>14</td><td>0</td><td>0</td></tr><tr><th>25</th><td>11.3</td><td>0.0668337</td><td>257.296</td><td>14</td><td>0</td><td>0</td></tr><tr><th>26</th><td>20.75</td><td>0.0251697</td><td>150.473</td><td>14</td><td>0</td><td>0</td></tr><tr><th>27</th><td>9.8</td><td>0.106817</td><td>114.249</td><td>15</td><td>0</td><td>0</td></tr><tr><th>28</th><td>19.1</td><td>0.0258673</td><td>147.642</td><td>9</td><td>0</td><td>0</td></tr><tr><th>29</th><td>5.985</td><td>0.0961856</td><td>127.168</td><td>4</td><td>0</td><td>0</td></tr><tr><th>30</th><td>18.35</td><td>0.09931</td><td>91.4462</td><td>11</td><td>0</td><td>0</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& Item\\_Weight & Item\\_Visibility & Item\\_MRP & Years\\_Opened & Item\\_Type\\_Baking Goods & Item\\_Type\\_Breads & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Int64 & Int64 & Int64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 14.6 & 0.0594052 & 179.698 & 16 & 0 & 0 & $\\dots$ \\\\\n",
       "\t2 & 16.25 & 0.025777 & 169.947 & 14 & 0 & 0 & $\\dots$ \\\\\n",
       "\t3 & 12.7929 & 0.0907781 & 153.102 & 28 & 0 & 0 & $\\dots$ \\\\\n",
       "\t4 & 13.85 & 0.0259367 & 164.921 & 14 & 0 & 0 & $\\dots$ \\\\\n",
       "\t5 & 9.195 & 0.0518271 & 77.4644 & 4 & 1 & 0 & $\\dots$ \\\\\n",
       "\t6 & 12.7929 & -0.969538 & 52.6666 & 28 & 0 & 0 & $\\dots$ \\\\\n",
       "\t7 & 14.65 & 0.0721787 & 262.959 & 14 & 0 & 0 & $\\dots$ \\\\\n",
       "\t8 & 9.6 & 0.161182 & 165.316 & 15 & 0 & 0 & $\\dots$ \\\\\n",
       "\t9 & 16.85 & 0.159826 & 195.548 & 9 & 0 & 0 & $\\dots$ \\\\\n",
       "\t10 & 6.03 & 0.0226994 & 176.403 & 9 & 0 & 0 & $\\dots$ \\\\\n",
       "\t11 & 12.7929 & 0.0269333 & 78.467 & 28 & 0 & 0 & $\\dots$ \\\\\n",
       "\t12 & 11.6 & 0.0387368 & 55.8272 & 6 & 0 & 0 & $\\dots$ \\\\\n",
       "\t13 & 12.7929 & 0.11225 & 123.341 & 28 & 0 & 0 & $\\dots$ \\\\\n",
       "\t14 & 12.7929 & -0.969538 & 240.622 & 28 & 0 & 0 & $\\dots$ \\\\\n",
       "\t15 & 15.1 & 0.0259934 & 147.408 & 16 & 0 & 0 & $\\dots$ \\\\\n",
       "\t16 & 6.325 & 0.125688 & 100.904 & 4 & 0 & 0 & $\\dots$ \\\\\n",
       "\t17 & 12.6 & 0.0627978 & 103.999 & 14 & 0 & 0 & $\\dots$ \\\\\n",
       "\t18 & 12.7929 & 0.0652031 & 166.082 & 28 & 0 & 0 & $\\dots$ \\\\\n",
       "\t19 & 20.7 & 0.169776 & 184.427 & 9 & 0 & 0 & $\\dots$ \\\\\n",
       "\t20 & 10.8 & 0.0498199 & 243.214 & 11 & 0 & 0 & $\\dots$ \\\\\n",
       "\t21 & 12.3 & 0.00942961 & 71.338 & 11 & 1 & 0 & $\\dots$ \\\\\n",
       "\t22 & 8.155 & 0.119351 & 190.153 & 26 & 0 & 0 & $\\dots$ \\\\\n",
       "\t23 & 12.7929 & -0.969538 & 171.742 & 28 & 0 & 0 & $\\dots$ \\\\\n",
       "\t24 & 7.905 & 0.0100279 & 249.641 & 14 & 0 & 0 & $\\dots$ \\\\\n",
       "\t25 & 11.3 & 0.0668337 & 257.296 & 14 & 0 & 0 & $\\dots$ \\\\\n",
       "\t26 & 20.75 & 0.0251697 & 150.473 & 14 & 0 & 0 & $\\dots$ \\\\\n",
       "\t27 & 9.8 & 0.106817 & 114.249 & 15 & 0 & 0 & $\\dots$ \\\\\n",
       "\t28 & 19.1 & 0.0258673 & 147.642 & 9 & 0 & 0 & $\\dots$ \\\\\n",
       "\t29 & 5.985 & 0.0961856 & 127.168 & 4 & 0 & 0 & $\\dots$ \\\\\n",
       "\t30 & 18.35 & 0.09931 & 91.4462 & 11 & 0 & 0 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m6392×48 DataFrame\u001b[0m\n",
       "\u001b[1m  Row \u001b[0m│\u001b[1m Item_Weight \u001b[0m\u001b[1m Item_Visibility \u001b[0m\u001b[1m Item_MRP \u001b[0m\u001b[1m Years_Opened \u001b[0m\u001b[1m Item_Type_Baking\u001b[0m ⋯\n",
       "\u001b[1m      \u001b[0m│\u001b[90m Float64     \u001b[0m\u001b[90m Float64         \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Int64        \u001b[0m\u001b[90m Int64           \u001b[0m ⋯\n",
       "──────┼─────────────────────────────────────────────────────────────────────────\n",
       "    1 │     14.6           0.0594052  179.698             16                   ⋯\n",
       "    2 │     16.25          0.025777   169.947             14\n",
       "    3 │     12.7929        0.0907781  153.102             28\n",
       "    4 │     13.85          0.0259367  164.921             14\n",
       "    5 │      9.195         0.0518271   77.4644             4                   ⋯\n",
       "    6 │     12.7929       -0.969538    52.6666            28\n",
       "    7 │     14.65          0.0721787  262.959             14\n",
       "    8 │      9.6           0.161182   165.316             15\n",
       "    9 │     16.85          0.159826   195.548              9                   ⋯\n",
       "   10 │      6.03          0.0226994  176.403              9\n",
       "   11 │     12.7929        0.0269333   78.467             28\n",
       "  ⋮   │      ⋮              ⋮            ⋮           ⋮                  ⋮      ⋱\n",
       " 6383 │     20.25          0.0260059  180.998             11\n",
       " 6384 │      8.235         0.0827636  146.508              4                   ⋯\n",
       " 6385 │     20.1           0.0224066  225.904             26\n",
       " 6386 │      5.63          0.0245413  105.131             16\n",
       " 6387 │     12.7929        0.0323813  166.184             28\n",
       " 6388 │     18.85          0.0679535  119.044             16                   ⋯\n",
       " 6389 │      5.88          0.0302422  101.799              9\n",
       " 6390 │     11.65          0.059063   172.042              6\n",
       " 6391 │      5.845         0.105181   214.422             14\n",
       " 6392 │      5.785         0.0405871  180.366              6                   ⋯\n",
       "\u001b[36m                                                44 columns and 6371 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stable_y=stable_all_train_val_proc[:,4]\n",
    "\n",
    "stable_x=select!(stable_all_train_val_proc,Not(:Item_Outlet_Sales))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "155200bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8523-element Vector{Float64}:\n",
       " 3735.138\n",
       "  443.4228\n",
       " 2097.27\n",
       "  732.38\n",
       "  994.7052\n",
       "  556.6088\n",
       "  343.5528\n",
       " 4022.7636\n",
       " 1076.5986\n",
       " 4710.535\n",
       " 1516.0266\n",
       " 2187.153\n",
       " 1589.2646\n",
       "    ⋮\n",
       " 4207.856\n",
       " 2479.4392\n",
       "  595.2252\n",
       "  468.7232\n",
       " 1571.288\n",
       "  858.882\n",
       " 3608.636\n",
       " 2778.3834\n",
       "  549.285\n",
       " 1193.1136\n",
       " 1845.5976\n",
       "  765.67"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stable_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23af41a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stable_x = CSV.read(\"stable_.csv\", DataFrame,header=true);\n",
    "#stable_y = CSV.read(\"train_processed_Y.csv\", DataFrame,header=true);\n",
    "#stableX_test = CSV.read(\"test_processed_without_Y.csv\", DataFrame,header=false);\n",
    "#stableY_test = CSV.read(\"test_processed_Y.csv\", DataFrame,header=false);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9089a54",
   "metadata": {},
   "source": [
    "# Accuracy Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "539c53b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_mse (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compute_mse(X, y, beta,beta0) #inc beta0\n",
    "    n,p = size(X)\n",
    "    beta_0= [beta0 for i = 1:n]\n",
    "    \n",
    "    return (sum((beta0.+Matrix(X)*beta .- Array(y)).^2)/n)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "989f6fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_r2 (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compute_r2(X, y, beta,beta0) #inc beta0\n",
    "    \n",
    "    beta_0= [beta0 for i = 1:n]\n",
    "    \n",
    "    SSres = sum( (Array(y) .- Matrix(X)*beta.+beta0).^2 )\n",
    "    SStot = sum( (y .- Statistics.mean(Array(y))).^2 )\n",
    "    return 1-SSres/SStot\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abeffa",
   "metadata": {},
   "source": [
    "# 3.a Robust Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daad4d4",
   "metadata": {},
   "source": [
    "\n",
    "$$\\min_{\\beta} \\sum_{i=1}^n | y_i-\\beta_0-\\beta^Tx_i| + \\lambda \\sum_{i=0}^p |\\beta_i|.$$\n",
    "\n",
    "We rewrite the above problem as:\n",
    "$$\\min_{\\beta,t,a} \\quad \\sum_{i=1}^n t_i + \\lambda \\sum_{j=0}^p a_j\n",
    "\\quad \\text{s.t.} \\\\\n",
    "\\quad \\ (y-\\beta_0-\\beta^Tx_i)\\ \\leq t_i,\\\\\n",
    "-\\quad \\ (y-\\beta_0-\\beta^Tx_i)\\ \\leq t_i,\\\\\n",
    "\\quad \\beta_j \\leq a_j,\\\\\n",
    "\\quad -\\beta_j \\leq a_j.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79a88896",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Gurobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04c9f25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only - expires 2022-08-18\r\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$ \\begin{alignat*}{1}\\text{feasibility}\\\\\n",
       "\\text{Subject to} \\quad\\end{alignat*}\n",
       " $$"
      ],
      "text/plain": [
       "A JuMP Model\n",
       "Feasibility problem with:\n",
       "Variables: 0\n",
       "Model mode: AUTOMATIC\n",
       "CachingOptimizer state: EMPTY_OPTIMIZER\n",
       "Solver name: Gurobi"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(Gurobi.Optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3414d967",
   "metadata": {},
   "source": [
    "# 1.3 a) Regression 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a30224ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_vals=[0.01, 0.03, 0.08, 0.1, 0.3, 0.8, 1, 3];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4ec1a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only - expires 2022-08-18\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "l1_regression (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gurobi_env=Gurobi.Env()\n",
    "\n",
    "function l1_regression(X, y, rho; solver_output=0)\n",
    "    n,p = size(X)\n",
    "    \n",
    "    # Build model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer,gurobi_env))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", solver_output) \n",
    "    \n",
    "    # Insert variables\n",
    "    @variable(model,beta[j=1:p])\n",
    "    @variable(model,a[j=1:p]>=0) \n",
    "    @variable(model, t[j=1:n]>=0) \n",
    "    @variable(model, d>=0)\n",
    "    @variable(model, beta0) #\n",
    "    \n",
    "    #Insert constraints\n",
    "    @constraint(model,[j=1:p], beta[j]<=a[j])\n",
    "    @constraint(model,[j=1:p], -beta[j]<=a[j])\n",
    "    @constraint(model, beta0<=d)\n",
    "    @constraint(model, -beta0<=d)\n",
    "    @constraint(model, [z=1:n],(((y[z,1]-beta0-transpose(beta)*Vector(X[z,:])))) <= t[z]) #X[z,:] 9 elements in row of X_z\n",
    "    @constraint(model,[z=1:n], -(((y[z,1]-beta0-transpose(beta)*Vector(X[z,:])))) <= t[z])\n",
    "    \n",
    "    #Objective\n",
    "    @objective(model,Min, sum(t[z] for z=1:n) + rho*(sum(a[j] for j=1:p)+d) )\n",
    "    \n",
    "    # Optimize\n",
    "    optimize!(model)\n",
    "    \n",
    "    # Return estimated betas\n",
    "    return (value.(beta0),value.(beta))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e469baf3",
   "metadata": {},
   "source": [
    "# 1.3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8653be1",
   "metadata": {},
   "source": [
    "### 1.3b i) For the data from the train and valid files, randomly split it into 70% training and 30% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "240b3f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "robust_regression_valid (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function robust_regression_valid(X, y,seed, rho_vals; method=l1_regression, split_at=0.7, solver_output=0)\n",
    "    n,p = size(X) #3132, 7\n",
    "    Random.seed!(seed)\n",
    "    split = convert(Int,floor(split_at*n)) #floor takes the integer part\n",
    "    \n",
    "    \n",
    "    #To create train and validation data, we will define the indices of each data.\n",
    "    permuted_indices = randperm(n)\n",
    "    train_indices, valid_indices = permuted_indices[1:split], permuted_indices[split+1:end]\n",
    "\n",
    "    X_train, y_train = X[train_indices,:], y[train_indices,:]\n",
    "    \n",
    "    X_valid, y_valid = X[valid_indices,:], y[valid_indices,:]\n",
    "\n",
    "    #create an array to hold the results\n",
    "    errors = zeros(length(rho_vals))\n",
    "    \n",
    "    for (i,rho) in enumerate(rho_vals)\n",
    "        #get the beta coefficients from the l1_regression\n",
    "        \n",
    "        \n",
    "        beta0,beta = method(X_train,y_train,rho,solver_output=solver_output)\n",
    "        \n",
    "        #compute the MSE with the optimal beta we just found\n",
    "        errors[i] = compute_mse(X_valid, y_valid, beta,beta0)\n",
    "    \n",
    "    end\n",
    "    \n",
    "    #get the best performing rho\n",
    "    \n",
    "    i_best = argmin(errors)\n",
    "    best_rho=rho_vals[i_best]\n",
    "    \n",
    "    #get the betas with best rho\n",
    "    best_betas = method(X,y,best_rho)\n",
    "    \n",
    "    return best_betas,best_rho, errors\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717d4a4",
   "metadata": {},
   "source": [
    "### 1.3 b) ii) Use the validation set to select an optimal value for the parameter λ in the set {0.01, 0.03, 0.08, 0.1, 0.3, 0.8, 1, 3}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98682ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zeroed features = 17\n",
      "Optimal rho: 3.0\n"
     ]
    }
   ],
   "source": [
    "beta_l1, rho_l1, errors_l1 = robust_regression_valid(stable_x,stable_y,1,rho_vals;method=l1_regression)\n",
    "\n",
    "beta_l1=pushfirst!(beta_l1[2],beta_l1[1])\n",
    "\n",
    "println(\"Number of zeroed features = \",(sum(abs.(beta_l1).<1e-4)))\n",
    "\n",
    "#println(\"MSE: \", compute_mse(stableX_test,stableY_test,beta_l1[2:8],beta_l1[1]))\n",
    "\n",
    "println(\"Optimal rho: \", rho_l1)\n",
    "\n",
    "#println(\"Betas:\",beta_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a57d664",
   "metadata": {},
   "source": [
    "### 1.3 b) iii) Try recombining the training and validation sets and then splitting them up in a different manner a few times (at least 5), and then training/validating a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d098136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zeroed features for test model 1: 17\n",
      "Number of zeroed features for test model 2: 12\n",
      "Number of zeroed features for test model 3: 17\n",
      "Number of zeroed features for test model 4: 17\n",
      "Number of zeroed features for test model 5: 17\n",
      "Number of zeroed features for test model 6: 11\n",
      "Number of zeroed features for test model 7: 12\n",
      "Number of zeroed features for test model 8: 17\n",
      "Number of zeroed features for test model 9: 12\n",
      "Number of zeroed features for test model 10: 17\n"
     ]
    }
   ],
   "source": [
    "MSE_Test_Models=zeros(10)\n",
    "\n",
    "p=10\n",
    "for i =1:p\n",
    "    #Recombines training and validation sets\n",
    "    #Splits them differenetly with different seeds\n",
    "    #Trains and validates a new model returning best betas from training set and rho from validation\n",
    "    beta_l1_cv, rho_l1_cv, errors_l1 = robust_regression_valid(stable_x,stable_y,i,rho_vals;method=l1_regression)\n",
    "    \n",
    "    beta_l1_cv2=pushfirst!(beta_l1_cv[2],beta_l1_cv[1])\n",
    "    \n",
    "    println(\"Number of zeroed features for test model \", i ,\": \",(sum(abs.(beta_l1_cv2).<1e-4)))\n",
    "    \n",
    "    #Calculate MSE on Test Set for best Rho in each of the 10 splits\n",
    "    \n",
    "    #MSE_Test_Model=compute_mse(stableX_test, stableY_test, beta_l1_cv2[2:8],beta_l1_cv2[1])\n",
    "    \n",
    "    #println(\"Validated lambda for test model \", i ,\": \", rho_l1_cv)\n",
    "    #println(\"MSE for test model \", i , \": \", MSE_Test_Model,[i])\n",
    "\n",
    "    #MSE_Test_Models[i]=MSE_Test_Model\n",
    "    \n",
    "end\n",
    "\n",
    "# range of (MSEs)  on the test data for each of these models\n",
    "\n",
    "#n  println(\"Largest MSE from Test Models \",MSE_Test_Models[argmax(MSE_Test_Models)])\n",
    "#println(\"Smallest MSE from Test Models \",MSE_Test_Models[argmin(MSE_Test_Models)])\n",
    "#println(\"Average MSE from Test Models \",mean(MSE_Test_Models))\n",
    "\n",
    "#println(\"Range for Test MSE: \", MSE_Test_Models[argmax(MSE_Test_Models)] - MSE_Test_Models[argmin(MSE_Test_Models)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf4f821",
   "metadata": {},
   "source": [
    "### 1.3 b) iii) What is the range of Mean Squared Errors (MSEs) you get on the test data for each of these models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe0f03e",
   "metadata": {},
   "source": [
    "Largest MSE from Test Models 5.024122859285739\n",
    "\n",
    "Smallest MSE from Test Models 4.88788585345981\n",
    "\n",
    "Average MSE from Test Models 4.956380782543436\n",
    "\n",
    "Range for Test MSE: 0.13623700582592857"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec066061",
   "metadata": {},
   "source": [
    "# 3.1 c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b29b4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only - expires 2022-08-18\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "l2_regression2 (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gurobi_env=Gurobi.Env()\n",
    "\n",
    "function l2_regression2(X, y, rho,split_at; solver_output=0)\n",
    "    n,p = size(X)\n",
    "    \n",
    "    \n",
    "    k = convert(Int,floor(split_at*n)) #floor takes the integer part\n",
    "    \n",
    "    # Build model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer,gurobi_env))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", solver_output) \n",
    "    \n",
    "    # Insert variables\n",
    "    @variable(model,beta[j=1:p])\n",
    "    @variable(model, beta0) \n",
    "    @variable(model,theta)\n",
    "    \n",
    "    @variable(model,a[j=1:p]) \n",
    "    @variable(model, u[i=1:n]>=0) \n",
    "    @variable(model, d>=0)\n",
    " \n",
    "    \n",
    "    #Insert constraints   \n",
    "    @constraint(model,[j=1:p], beta[j]<=a[j])   \n",
    "    @constraint(model,[j=1:p], -(beta[j])<=a[j])\n",
    "    @constraint(model, [i=1:n], ( (y[i,1]-beta0-transpose(beta)*Vector(X[i,:])) ) <= theta+u[i])  \n",
    "    @constraint(model,[i=1:n], -( (y[i,1]-beta0-transpose(beta)*Vector(X[i,:]) )) <= theta+u[i])\n",
    "    @constraint(model,[i=1:n],u[i]>=0)\n",
    "    @constraint(model, beta0<=d)\n",
    "    @constraint(model, -beta0<=d) \n",
    "    \n",
    "    #Objective\n",
    "    @objective(model,Min, k*theta+sum(u[i] for i=1:n) + rho*(sum(a[j] for j=1:p)) )   \n",
    "    \n",
    "    # Optimize\n",
    "    optimize!(model)\n",
    "    \n",
    "    # Return estimated betas\n",
    "    beta0=value.(beta0)    \n",
    "    beta=value.(beta)\n",
    "\n",
    "    \n",
    "    errors=zeros(n) \n",
    "    beta_0= [beta0 for i = 1:n]\n",
    "       \n",
    "    errors= ((beta0.+Matrix(X)*beta .- Array(y)).^2) .^0.5\n",
    "    \n",
    "    #Sort indices of MSE's\n",
    "    indicies=sortperm(errors[:,1])\n",
    "    \n",
    "    #for (i,rho) in enumerate(indicies)\n",
    "    #    print(i,rho)\n",
    "    #end\n",
    "    \n",
    "    valid_indices=indicies[1:n-k] \n",
    "    training_indices=indicies[1+n-k:n]\n",
    "      \n",
    "    x_valid_set, y_valid_set = X[valid_indices,:], y[valid_indices,:]\n",
    "    x_training_set,y_training_set=X[training_indices,:],y[training_indices,:]\n",
    "   \n",
    "    \n",
    "    return (value.(beta0),value.(beta),errors,x_valid_set,y_valid_set,x_training_set,y_training_set)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1f33c",
   "metadata": {},
   "source": [
    "# 3.d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07d65b",
   "metadata": {},
   "source": [
    "### 3.d i) ii) iii) CREATING AND EXPORT TOUGHEST SUBSET | Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c408c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect the toughest training set\n",
    "\n",
    "beta0,beta,errors,x_valid_set,y_valid_set,x_training_set,y_training_set= l2_regression2(stable_x,stable_y,.01,.7);\n",
    "x_training_set\n",
    "y_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6e3bf01",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Item_Weight</th><th>Item_Visibility</th><th>Item_MRP</th><th>Years_Opened</th><th>Item_Type_Baking Goods</th><th>Item_Type_Breads</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Int64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>4,474 rows × 48 columns (omitted printing of 42 columns)</p><tr><th>1</th><td>7.155</td><td>0.168527</td><td>33.6874</td><td>11</td><td>0</td><td>1</td></tr><tr><th>2</th><td>12.35</td><td>0.0418025</td><td>34.8216</td><td>14</td><td>0</td><td>0</td></tr><tr><th>3</th><td>10.85</td><td>0.0480043</td><td>117.581</td><td>15</td><td>0</td><td>0</td></tr><tr><th>4</th><td>9.3</td><td>0.0422563</td><td>124.439</td><td>26</td><td>0</td><td>0</td></tr><tr><th>5</th><td>12.15</td><td>0.062276</td><td>37.9532</td><td>9</td><td>0</td><td>0</td></tr><tr><th>6</th><td>9.0</td><td>0.0692291</td><td>55.9614</td><td>11</td><td>0</td><td>0</td></tr><tr><th>7</th><td>16.75</td><td>0.0137113</td><td>100.267</td><td>15</td><td>0</td><td>0</td></tr><tr><th>8</th><td>12.7929</td><td>0.0977687</td><td>142.45</td><td>28</td><td>0</td><td>0</td></tr><tr><th>9</th><td>6.385</td><td>0.140328</td><td>109.16</td><td>15</td><td>0</td><td>0</td></tr><tr><th>10</th><td>18.2</td><td>0.16296</td><td>43.5086</td><td>26</td><td>0</td><td>0</td></tr><tr><th>11</th><td>7.97</td><td>0.0400711</td><td>87.3514</td><td>11</td><td>0</td><td>0</td></tr><tr><th>12</th><td>8.71</td><td>0.139796</td><td>47.2376</td><td>4</td><td>0</td><td>0</td></tr><tr><th>13</th><td>8.85</td><td>0.112571</td><td>122.039</td><td>26</td><td>0</td><td>0</td></tr><tr><th>14</th><td>8.655</td><td>0.0883736</td><td>119.376</td><td>11</td><td>0</td><td>0</td></tr><tr><th>15</th><td>16.2</td><td>0.175898</td><td>183.761</td><td>4</td><td>0</td><td>0</td></tr><tr><th>16</th><td>12.7929</td><td>0.0253541</td><td>144.476</td><td>28</td><td>0</td><td>0</td></tr><tr><th>17</th><td>6.78</td><td>-0.941692</td><td>227.469</td><td>16</td><td>0</td><td>0</td></tr><tr><th>18</th><td>7.405</td><td>0.159844</td><td>206.63</td><td>4</td><td>0</td><td>0</td></tr><tr><th>19</th><td>17.6</td><td>0.127412</td><td>111.32</td><td>15</td><td>0</td><td>0</td></tr><tr><th>20</th><td>8.3</td><td>0.0382043</td><td>87.7198</td><td>9</td><td>0</td><td>0</td></tr><tr><th>21</th><td>12.7929</td><td>0.145952</td><td>160.955</td><td>28</td><td>0</td><td>0</td></tr><tr><th>22</th><td>20.7</td><td>0.027052</td><td>73.9354</td><td>6</td><td>0</td><td>0</td></tr><tr><th>23</th><td>18.0</td><td>0.124646</td><td>117.212</td><td>14</td><td>0</td><td>0</td></tr><tr><th>24</th><td>8.935</td><td>0.04041</td><td>52.9298</td><td>6</td><td>0</td><td>0</td></tr><tr><th>25</th><td>9.695</td><td>0.0292234</td><td>175.437</td><td>11</td><td>0</td><td>0</td></tr><tr><th>26</th><td>16.5</td><td>0.0126893</td><td>39.7506</td><td>4</td><td>0</td><td>0</td></tr><tr><th>27</th><td>12.7929</td><td>0.284066</td><td>105.562</td><td>28</td><td>0</td><td>0</td></tr><tr><th>28</th><td>10.5</td><td>0.0441396</td><td>143.913</td><td>15</td><td>1</td><td>0</td></tr><tr><th>29</th><td>12.7929</td><td>0.0440638</td><td>147.242</td><td>28</td><td>0</td><td>0</td></tr><tr><th>30</th><td>9.1</td><td>-1.00306</td><td>173.205</td><td>11</td><td>0</td><td>1</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& Item\\_Weight & Item\\_Visibility & Item\\_MRP & Years\\_Opened & Item\\_Type\\_Baking Goods & Item\\_Type\\_Breads & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Int64 & Int64 & Int64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 7.155 & 0.168527 & 33.6874 & 11 & 0 & 1 & $\\dots$ \\\\\n",
       "\t2 & 12.35 & 0.0418025 & 34.8216 & 14 & 0 & 0 & $\\dots$ \\\\\n",
       "\t3 & 10.85 & 0.0480043 & 117.581 & 15 & 0 & 0 & $\\dots$ \\\\\n",
       "\t4 & 9.3 & 0.0422563 & 124.439 & 26 & 0 & 0 & $\\dots$ \\\\\n",
       "\t5 & 12.15 & 0.062276 & 37.9532 & 9 & 0 & 0 & $\\dots$ \\\\\n",
       "\t6 & 9.0 & 0.0692291 & 55.9614 & 11 & 0 & 0 & $\\dots$ \\\\\n",
       "\t7 & 16.75 & 0.0137113 & 100.267 & 15 & 0 & 0 & $\\dots$ \\\\\n",
       "\t8 & 12.7929 & 0.0977687 & 142.45 & 28 & 0 & 0 & $\\dots$ \\\\\n",
       "\t9 & 6.385 & 0.140328 & 109.16 & 15 & 0 & 0 & $\\dots$ \\\\\n",
       "\t10 & 18.2 & 0.16296 & 43.5086 & 26 & 0 & 0 & $\\dots$ \\\\\n",
       "\t11 & 7.97 & 0.0400711 & 87.3514 & 11 & 0 & 0 & $\\dots$ \\\\\n",
       "\t12 & 8.71 & 0.139796 & 47.2376 & 4 & 0 & 0 & $\\dots$ \\\\\n",
       "\t13 & 8.85 & 0.112571 & 122.039 & 26 & 0 & 0 & $\\dots$ \\\\\n",
       "\t14 & 8.655 & 0.0883736 & 119.376 & 11 & 0 & 0 & $\\dots$ \\\\\n",
       "\t15 & 16.2 & 0.175898 & 183.761 & 4 & 0 & 0 & $\\dots$ \\\\\n",
       "\t16 & 12.7929 & 0.0253541 & 144.476 & 28 & 0 & 0 & $\\dots$ \\\\\n",
       "\t17 & 6.78 & -0.941692 & 227.469 & 16 & 0 & 0 & $\\dots$ \\\\\n",
       "\t18 & 7.405 & 0.159844 & 206.63 & 4 & 0 & 0 & $\\dots$ \\\\\n",
       "\t19 & 17.6 & 0.127412 & 111.32 & 15 & 0 & 0 & $\\dots$ \\\\\n",
       "\t20 & 8.3 & 0.0382043 & 87.7198 & 9 & 0 & 0 & $\\dots$ \\\\\n",
       "\t21 & 12.7929 & 0.145952 & 160.955 & 28 & 0 & 0 & $\\dots$ \\\\\n",
       "\t22 & 20.7 & 0.027052 & 73.9354 & 6 & 0 & 0 & $\\dots$ \\\\\n",
       "\t23 & 18.0 & 0.124646 & 117.212 & 14 & 0 & 0 & $\\dots$ \\\\\n",
       "\t24 & 8.935 & 0.04041 & 52.9298 & 6 & 0 & 0 & $\\dots$ \\\\\n",
       "\t25 & 9.695 & 0.0292234 & 175.437 & 11 & 0 & 0 & $\\dots$ \\\\\n",
       "\t26 & 16.5 & 0.0126893 & 39.7506 & 4 & 0 & 0 & $\\dots$ \\\\\n",
       "\t27 & 12.7929 & 0.284066 & 105.562 & 28 & 0 & 0 & $\\dots$ \\\\\n",
       "\t28 & 10.5 & 0.0441396 & 143.913 & 15 & 1 & 0 & $\\dots$ \\\\\n",
       "\t29 & 12.7929 & 0.0440638 & 147.242 & 28 & 0 & 0 & $\\dots$ \\\\\n",
       "\t30 & 9.1 & -1.00306 & 173.205 & 11 & 0 & 1 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m4474×48 DataFrame\u001b[0m\n",
       "\u001b[1m  Row \u001b[0m│\u001b[1m Item_Weight \u001b[0m\u001b[1m Item_Visibility \u001b[0m\u001b[1m Item_MRP \u001b[0m\u001b[1m Years_Opened \u001b[0m\u001b[1m Item_Type_Baking\u001b[0m ⋯\n",
       "\u001b[1m      \u001b[0m│\u001b[90m Float64     \u001b[0m\u001b[90m Float64         \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Int64        \u001b[0m\u001b[90m Int64           \u001b[0m ⋯\n",
       "──────┼─────────────────────────────────────────────────────────────────────────\n",
       "    1 │      7.155         0.168527    33.6874            11                   ⋯\n",
       "    2 │     12.35          0.0418025   34.8216            14\n",
       "    3 │     10.85          0.0480043  117.581             15\n",
       "    4 │      9.3           0.0422563  124.439             26\n",
       "    5 │     12.15          0.062276    37.9532             9                   ⋯\n",
       "    6 │      9.0           0.0692291   55.9614            11\n",
       "    7 │     16.75          0.0137113  100.267             15\n",
       "    8 │     12.7929        0.0977687  142.45              28\n",
       "    9 │      6.385         0.140328   109.16              15                   ⋯\n",
       "   10 │     18.2           0.16296     43.5086            26\n",
       "   11 │      7.97          0.0400711   87.3514            11\n",
       "  ⋮   │      ⋮              ⋮            ⋮           ⋮                  ⋮      ⋱\n",
       " 4465 │     20.25          0.0363997  219.348             11\n",
       " 4466 │     12.7929        0.0345844  248.375             28                   ⋯\n",
       " 4467 │      5.695         0.0659609  259.265              6\n",
       " 4468 │     12.7929        0.0306933  228.035             28\n",
       " 4469 │      6.825         0.059847   262.523             16\n",
       " 4470 │     12.6           0.0743386  255.536              6                   ⋯\n",
       " 4471 │     12.7929        0.0888399  254.267             28\n",
       " 4472 │     12.7929       -0.969538   253.036             28\n",
       " 4473 │     12.7929        0.0142956  242.651             28\n",
       " 4474 │     12.7929        0.0105509  234.996             28                   ⋯\n",
       "\u001b[36m                                                44 columns and 4453 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88255935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4474×1 Matrix{Float64}:\n",
       "  1023.3346\n",
       "   311.5944\n",
       "   351.5424\n",
       "  1609.9044\n",
       "  1078.596\n",
       "   386.8298\n",
       "   203.7348\n",
       "   564.5984\n",
       "   107.8596\n",
       "  1070.6064\n",
       "  1062.6168\n",
       "   910.8144\n",
       "  1609.9044\n",
       "     ⋮\n",
       " 10072.8882\n",
       "  8994.958\n",
       "  8323.8316\n",
       " 10236.675\n",
       "  9275.9256\n",
       " 10306.584\n",
       "  9779.9362\n",
       "  9664.7528\n",
       " 10993.6896\n",
       " 11445.102\n",
       " 12117.56\n",
       " 13086.9648"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4c6ca5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"y_training_stable.csv\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Tables\n",
    "CSV.write( \"x_training_stable.csv\",  x_training_set)\n",
    "CSV.write(\"y_training_stable.csv\",  Tables.table(y_training_set), writeheader=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78037ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461938ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac35efbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for lambda 0.01 on validation set: 35236.91965583911\n",
      "MSE for lambda 0.03 on validation set: 35236.91965583911\n",
      "MSE for lambda 0.08 on validation set: 35236.59924366356\n",
      "MSE for lambda 0.1 on validation set: 35236.59924366356\n",
      "MSE for lambda 0.3 on validation set: 35247.60705311133\n",
      "MSE for lambda 0.8 on validation set: 35276.847146715314\n",
      "MSE for lambda 1.0 on validation set: 35200.44920561531\n",
      "MSE for lambda 3.0 on validation set: 35293.20320378044\n",
      "Lambda with lowest MSE: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1213.918935098964, [0.8467673274327916, -48.971670709658014, 14.480589422280781, -13.959939921318218, -13.483067441165531, 31.426580567747123, -154.99498501070866, 0.0, -134.9223124231603, -74.28427140915664  …  -30.491275987267045, -16.51082468940278, -5.120080816921965, -6.467144518637137, 116.98484536202608, 0.0, 0.0, 4.637132924641932, 59.9593455826016, -51.40257179248158])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors1 = zeros(length(rho_vals))\n",
    "rho_errors= zeros(length(rho_vals))\n",
    "MSE_list= zeros(length(rho_vals))\n",
    "\n",
    "\n",
    "\n",
    "for (i,rho) in enumerate(rho_vals)\n",
    "    beta0,beta,errors,x_valid_set,y_valid_set,x_training_set,y_training_set= l2_regression2(stable_x,stable_y,rho,.7)#,#points in training set)\n",
    "    \n",
    "    \n",
    "    #Calculate MSE on validation set for each rho\n",
    "    errors1[i] = compute_mse(x_valid_set,y_valid_set, beta,beta0)\n",
    "\n",
    "\n",
    "\n",
    "end  \n",
    "\n",
    "#MSE on validation set for each rho\n",
    "for (i,rho) in enumerate(rho_vals)\n",
    "    println(\"MSE for lambda \",rho, \" on validation set: \" ,errors1[i])\n",
    "\n",
    "end\n",
    "\n",
    "#Choose rho corresponding to the lowest MSE as your optimal \n",
    "println(\"Lambda with lowest MSE: \" , rho_vals[argmin(errors1)])\n",
    "\n",
    "#use the function from part (a) on  combined training/validation set with the optimal λ\n",
    "beta0,beta=l1_regression(stable_x,stable_y, rho_vals[argmin(errors1)]; solver_output=0)\n",
    "\n",
    "#MSE on model on test data\n",
    "\n",
    "###FinalMSE = compute_mse(stableX_test, stableY_test, beta,beta0)\n",
    "\n",
    "###println(\"MSE on test data using Lambda with lowest MSE from above: \", FinalMSE)\n",
    "\n",
    "#print(\"rho & MSE:\", errors) #why errors is bigger than just 1 mse for each lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e95986",
   "metadata": {},
   "source": [
    "Lambda with lowest MSE: 0.1\n",
    "\n",
    "MSE on test data using Lambda with lowest MSE from above: 4.891723552391911"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f9717",
   "metadata": {},
   "source": [
    "### 1.3 d iv) Compare to your results from part (b) – how do the MSEs compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f759f3",
   "metadata": {},
   "source": [
    "The MSE of this new model, taking the optimal lambda as 0.1, is 4.891723552391911.\n",
    "\n",
    "Compared to the results in part b) where\n",
    "\n",
    "The largest Mean Squared Error (MSE) from Test Models is 5.024122859285739\n",
    "\n",
    "The smallest MSE from Test Models is 4.88788585345981\n",
    "\n",
    "The range for Test MSE is 0.13623700582592857\n",
    "\n",
    "We see that we are at the lower bound of the Mean Squared Errors for our test data.\n",
    "        \n",
    "Compared to the average MSE for the test set in part b of 4.956380782543436 (obtained after splitting the training and \n",
    "validation set in 10 different ways) we see that our final test MSE using this robust optimisation is lower than this mean also.\n",
    "    \n",
    "It may be the case that, given a different test set, repeating part b may give a different minimun test MSE which is \n",
    "slightly higher or lower than that given in part b, but our test MSE using this stable robust method should consistently be lower than the\n",
    "mean test MSE obtained though finding lambda and betas from splitting the data randomly. This is another benefit of incoorporating robust stable regression.\n",
    "\n",
    "Using stable regression for hyperparameter tuning produces more consistent optimal lambdas (0.01) than those produced\n",
    "randomly splitting the data (0.1,0.03,0.1,0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a087e6",
   "metadata": {},
   "source": [
    "Interestingly, if we use the betas obtained from the robust model (that created the training set on the toughest 70% of training and validation \n",
    "data) that produces the lowest MSE with the associated lambda on the test data, we obtain a MSE of 4.854629988706049, which is \n",
    "slightly lower than that Test MSE obtained using the same optimal lambda and betas found using the training and validation set \n",
    "combined (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "86906140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on test data using optimal Lambda (0.1) and betas from toughest training set: 4.854629988706049\n"
     ]
    }
   ],
   "source": [
    "#Using the optimal lambda (0.1) obtained above in the robust regression and applying the betas obtained on the test set\n",
    "\n",
    "beta0,beta,errors,x_valid_set,y_valid_set,x_training_set,y_training_set= l2_regression2(stable_x,stable_y,0.1,.7)#,#points in training set)\n",
    "\n",
    "FinalMSE_betas2 = compute_mse(stableX_test, stableY_test, beta,beta0)\n",
    "\n",
    "println(\"MSE on test data using optimal Lambda (0.1) and betas from toughest training set: \", FinalMSE_betas2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
